{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe712b1",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e88d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3981cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer, wordpunct_tokenize, TweetTokenizer, MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac84106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Success doesn't come from what you do occasionally but what you do consistently.\"\n",
    "text2 = \"Future belongs to those who believe in the beauty of their dreams.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84650c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens : ['Success', 'does', \"n't\", 'come', 'from', 'what', 'you', 'do', 'occasionally', 'but', 'what', 'you', 'do', 'consistently', '.']\n",
      "\n",
      "Sentence Token : [\"Success doesn't come from what you do occasionally but what you do consistently.\"]\n",
      "\n",
      "Character Tokenize : ['S', 'u', 'c', 'c', 'e', 's', 's', ' ', 'd', 'o', 'e', 's', 'n', \"'\", 't', ' ', 'c', 'o', 'm', 'e', ' ', 'f', 'r', 'o', 'm', ' ', 'w', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'd', 'o', ' ', 'o', 'c', 'c', 'a', 's', 'i', 'o', 'n', 'a', 'l', 'l', 'y', ' ', 'b', 'u', 't', ' ', 'w', 'h', 'a', 't', ' ', 'y', 'o', 'u', ' ', 'd', 'o', ' ', 'c', 'o', 'n', 's', 'i', 's', 't', 'e', 'n', 't', 'l', 'y', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenize\n",
    "word_token1 = word_tokenize(text1)\n",
    "print(\"Word Tokens :\", word_token1)\n",
    "print()\n",
    "#Sent Tokenize\n",
    "sent_token1 = sent_tokenize(text1)\n",
    "print(\"Sentence Token :\", sent_token1)\n",
    "print()\n",
    "#Character Tokenize\n",
    "char_token1 = list(text1)\n",
    "print(\"Character Tokenize :\", char_token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d1ac27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words : 15\n",
      "Number of sentence : 1\n",
      "Number of characters : 80\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words :\",len(word_token1))\n",
    "print(\"Number of sentence :\", len(sent_token1))\n",
    "print(\"Number of characters :\", len(char_token1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "416196a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens : ['Future', 'belongs', 'to', 'those', 'who', 'believe', 'in', 'the', 'beauty', 'of', 'their', 'dreams', '.']\n",
      "\n",
      "Sentence Token : ['Future belongs to those who believe in the beauty of their dreams.']\n",
      "\n",
      "Character Tokenize : ['F', 'u', 't', 'u', 'r', 'e', ' ', 'b', 'e', 'l', 'o', 'n', 'g', 's', ' ', 't', 'o', ' ', 't', 'h', 'o', 's', 'e', ' ', 'w', 'h', 'o', ' ', 'b', 'e', 'l', 'i', 'e', 'v', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'b', 'e', 'a', 'u', 't', 'y', ' ', 'o', 'f', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'd', 'r', 'e', 'a', 'm', 's', '.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenize\n",
    "word_token2 = word_tokenize(text2)\n",
    "print(\"Word Tokens :\", word_token2)\n",
    "print()\n",
    "\n",
    "#Sent Tokenize\n",
    "sent_token2 = sent_tokenize(text2)\n",
    "print(\"Sentence Token :\", sent_token2)\n",
    "print()\n",
    "\n",
    "#Character Tokenize\n",
    "char_token2 = list(text2)\n",
    "print(\"Character Tokenize :\", char_token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f24adb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words : 13\n",
      "Number of sentence : 1\n",
      "Number of characters : 66\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words :\",len(word_token2))\n",
    "print(\"Number of sentence :\", len(sent_token2))\n",
    "print(\"Number of characters :\", len(char_token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425a61ad",
   "metadata": {},
   "source": [
    "## 1.1 White Space Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c2b48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30ccb180",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29d24907",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= \"Like a flower in the dessert, I have to grow in the cruelest weather. Holding on to every drop of rain, just to stay alive. But it‚Äôs not enough to survive, I want to bloom beneath the blazing sun. And show y'all the colours that live inside of me. I want you tsee what I can become. ‚ÄòChristy Ann Martine‚Äô\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5f79e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ede1b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Like', 'a', 'flower', 'in', 'the', 'dessert,', 'I', 'have', 'to', 'grow', 'in', 'the', 'cruelest', 'weather.', 'Holding', 'on', 'to', 'every', 'drop', 'of', 'rain,', 'just', 'to', 'stay', 'alive.', 'But', 'it‚Äôs', 'not', 'enough', 'to', 'survive,', 'I', 'want', 'to', 'bloom', 'beneath', 'the', 'blazing', 'sun.', 'And', 'show', \"y'all\", 'the', 'colours', 'that', 'live', 'inside', 'of', 'me.', 'I', 'want', 'you', 'tsee', 'what', 'I', 'can', 'become.', '‚ÄòChristy', 'Ann', 'Martine‚Äô']\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8788e7",
   "metadata": {},
   "source": [
    "## 1.2  NLTK Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a8b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hope, is the only thing stronger than fear! #Hope #Amal.M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe640c",
   "metadata": {},
   "source": [
    "1.2.1 Word SentenceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1543ef8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', '#', 'Hope', '#', 'Amal.M']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e60aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hope, is the only thing stronger than fear!', '#Hope #Amal.M']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980ec4c",
   "metadata": {},
   "source": [
    "1.2.2 Punctuation Based Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73f7ae01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'than', 'fear', '!', '#', 'Hope', '#', 'Amal', '.', 'M']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9818cc",
   "metadata": {},
   "source": [
    "1.2.3 TreeBank Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cac672fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"what you don't want to be done to yourself, don't do to others...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7f35b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'you', 'do', \"n't\", 'want', 'to', 'be', 'done', 'to', 'yourself', ',', 'do', \"n't\", 'do', 'to', 'others', '...']\n"
     ]
    }
   ],
   "source": [
    "token = TreebankWordTokenizer()\n",
    "print(token.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6586c36",
   "metadata": {},
   "source": [
    "1.2.4 Tweet Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a1029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"Don't take cryptocurrency advice from people on TwitterüòÇüòàüçåüåàüòÇ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8fda74aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'Twitter', 'üòÇ', 'üòà', 'üçå', 'üåà', 'üòÇ']\n"
     ]
    }
   ],
   "source": [
    "token = TweetTokenizer()\n",
    "print(token.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a78a0",
   "metadata": {},
   "source": [
    "1.2.5 MWE-Multi Word Tokenizer Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b310f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hope is only thing stronger than fear! Hunger Games #Hope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2cc038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hope', 'is', 'only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger', 'Games', '#', 'Hope']\n"
     ]
    }
   ],
   "source": [
    "token = MWETokenizer()\n",
    "print(token.tokenize(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b5623a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hope', 'is_only', 'thing', 'stronger', 'than', 'fear', '!', 'Hunger_Games', '#', 'Hope']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hope is only thing stronger than fear! Hunger Games #Hope\"\n",
    "token = MWETokenizer()\n",
    "token.add_mwe(('Hunger','Games'))\n",
    "token.add_mwe(('is','only'))\n",
    "print(token.tokenize(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58cb4df",
   "metadata": {},
   "source": [
    "## 1.3 TextBlob Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1a5e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3902da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"But I'm glad you'll see me as I am. Above all, I wouldn't want people to think that I want to prove anything\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63b4acb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['But', 'I', \"'m\", 'glad', 'you', \"'ll\", 'see', 'me', 'as', 'I', 'am', 'Above', 'all', 'I', 'would', \"n't\", 'want', 'people', 'to', 'think', 'that', 'I', 'want', 'to', 'prove', 'anything']\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(text)\n",
    "#Word Tokenization of text\n",
    "text_words = blob.words\n",
    "#to see all tokens\n",
    "print(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd97885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "#to count word tokens\n",
    "print(len(text_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d06cd35",
   "metadata": {},
   "source": [
    "## 1.4 Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67c1a573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import spacy\\nnlp = spacy.load(\"en_core_web_sm\")\\n\\ntext = \"All happy families are alike; each unhappy family is unhappy in its own way!!!üëåüèªüëåüèª#Leo Tolstoy\"\\ndoc = nlp(text)\\nfor token in doc:\\n    print(token, token.idk)'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"All happy families are alike; each unhappy family is unhappy in its own way!!!üëåüèªüëåüèª#Leo Tolstoy\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token, token.idk)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d16e64",
   "metadata": {},
   "source": [
    "## 1.5 Gensim Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "502adafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d29b1246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['But',\n",
       " 'I',\n",
       " 'm',\n",
       " 'glad',\n",
       " 'you',\n",
       " 'll',\n",
       " 'see',\n",
       " 'me',\n",
       " 'as',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Above',\n",
       " 'all',\n",
       " 'I',\n",
       " 'wouldn',\n",
       " 't',\n",
       " 'want',\n",
       " 'people',\n",
       " 'to',\n",
       " 'think',\n",
       " 'that',\n",
       " 'I',\n",
       " 'want',\n",
       " 'to',\n",
       " 'prove',\n",
       " 'anything']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857181b",
   "metadata": {},
   "source": [
    "## 1.6 Tokenization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b45bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "503dfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c0a36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3e5be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntoken = Tokenizer(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4eea7e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'happy', 'families', 'are', 'alike', 'each', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', 'üëåüèªüëåüèª', 'leo', 'tolstoy', '‚ù§', '‚ù§']\n"
     ]
    }
   ],
   "source": [
    "#To perform tokenization we use: text_to_word_sequence method from the Class Keras.preprocessing.text class. The great thing about Keras is converting the alphabet in a lower case before tokenizing it, which can be quite a time-saver.\n",
    "\n",
    "text = \"All happy families are alike; each unhappy family is unhappy in its own way!!!üëåüèªüëåüèª#Leo Tolstoy ‚ù§ ‚ù§\"\n",
    "ntoken.fit_on_texts(text)\n",
    "list_words = text_to_word_sequence(text)\n",
    "print(list_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a557bc",
   "metadata": {},
   "source": [
    "# 2. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de08f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a57a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7664bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02492928",
   "metadata": {},
   "outputs": [],
   "source": [
    "lan = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c01221a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'preprocess', 'techniqu', 'for', 'natur', 'langauag', 'process']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens = text.split()\n",
    "    stems = [stem.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = \"text preprocessing techniques for natural langauage processing\"\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e828a51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem',\n",
       " 'is',\n",
       " 'prefer',\n",
       " 'when',\n",
       " 'the',\n",
       " 'mean',\n",
       " 'of',\n",
       " 'the',\n",
       " 'word',\n",
       " 'is',\n",
       " 'not',\n",
       " 'import',\n",
       " 'for',\n",
       " 'analysi']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens = text.split()\n",
    "    stems = [snow.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = \"Stemming is preferred when the meaning of the word is not important for analysis\"\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16b93803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stem', 'is', 'a', 'simpl', 'and', 'fast', 'techn', 'comp', 'to', 'lem']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens = text.split()\n",
    "    stems = [lan.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = \"Stemming is a simpler and faster technique compared to lemmatization\"\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa78e93a",
   "metadata": {},
   "source": [
    "# 3. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1e1b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce6985ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29e5e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9a62e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52842642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'preprocessing',\n",
       " 'technique',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'langauage',\n",
       " 'processing']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens = text.split()\n",
    "    stems = [lemmas.lemmatize(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = \"text preprocessing techniques for natural langauage processing\"\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e495e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stemming',\n",
       " 'is',\n",
       " 'preferred',\n",
       " 'when',\n",
       " 'the',\n",
       " 'meaning',\n",
       " 'of',\n",
       " 'the',\n",
       " 'word',\n",
       " 'is',\n",
       " 'not',\n",
       " 'important',\n",
       " 'for',\n",
       " 'analysis']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens = text.split()\n",
    "    stems = [lemmas.lemmatize(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = \"Stemming is preferred when the meaning of the word is not important for analysis\"\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58a272d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stemming',\n",
       " 'is',\n",
       " 'a',\n",
       " 'simpler',\n",
       " 'and',\n",
       " 'faster',\n",
       " 'technique',\n",
       " 'compared',\n",
       " 'to',\n",
       " 'lemmatization']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem_words(text):\n",
    "    word_tokens = text.split()\n",
    "    stems = [lemmas.lemmatize(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = \"Stemming is a simpler and faster technique compared to lemmatization\"\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3a257",
   "metadata": {},
   "source": [
    "# 4. POS Tagging - (Part of Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d9eee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 11001] getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91e1c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79a17591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Keep', 'VB'), ('the', 'DT'), ('book', 'NN'), ('on', 'IN'), ('top', 'JJ'), ('shelf', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Keep the book on top shelf\"\n",
    "print(pos_tag(word_tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8092f74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('POS', 'NNP'), ('tagger', 'NN'), ('in', 'IN'), ('the', 'DT'), ('NLTK', 'NNP'), ('library', 'NN'), ('outputs', 'NNS'), ('specific', 'JJ'), ('tags', 'NNS'), ('for', 'IN'), ('certain', 'JJ'), ('words', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"The POS tagger in the NLTK library outputs specific tags for certain words.\"\n",
    "print(pos_tag(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9433857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('She', 'PRP'), ('sells', 'VBZ'), ('seashells', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('seashore', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "x = \"She sells seashells on the seashore\"\n",
    "print(pos_tag(word_tokenize(x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
